{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc5-mbsX9PZC"
   },
   "source": [
    "# AlphaFold2_advanced\n",
    "This notebook modifies deepmind's [original notebook](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb) to add experimental support for modeling complexes (both homo and hetero-oligomers), option to run MMseqs2 instead of Jackhmmer for MSA generation and advanced functionality.\n",
    "\n",
    "See [ColabFold](https://github.com/sokrypton/ColabFold/) for other related notebooks\n",
    "\n",
    "**Limitations**\n",
    "- This notebook does NOT use Templates.\n",
    "- For a typical Google-Colab session, with a `16G-GPU`, the max total length is **1400 residues**. Sometimes a `12G-GPU` is assigned, in which the max length is ~1000 residues.\n",
    "- Can I use the models for **Molecular Replacement**? Yes, but be CAREFUL, the bfactor column is populated with pLDDT confidence values (higher = better). Phenix.phaser expects a \"real\" bfactor, where (lower = better). See [post](https://twitter.com/cheshireminima/status/1423929241675120643) from Claudia Millán on how to process models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "woIxeCPygt7K"
   },
   "outputs": [],
   "source": [
    "#@title Install software\n",
    "#@markdown Please execute this cell by pressing the _Play_ button \n",
    "#@markdown on the left.\n",
    "use_amber_relax = False #@param {type:\"boolean\"}\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "import jax\n",
    "if jax.local_devices()[0].platform == 'tpu':\n",
    "  raise RuntimeError('Colab TPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
    "elif jax.local_devices()[0].platform == 'cpu':\n",
    "  raise RuntimeError('Colab CPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
    "\n",
    "from IPython.display import FileLink\n",
    "from IPython.utils import io\n",
    "import subprocess\n",
    "import tqdm.notebook\n",
    "\n",
    "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
    "\n",
    "########################################################################################\n",
    "# --- Python imports ---\n",
    "import colabfold as cf\n",
    "import pairmsa\n",
    "import sys\n",
    "import pickle\n",
    "if use_amber_relax:\n",
    "  sys.path.append('/opt/conda/lib/python3.6/site-packages')\n",
    "\n",
    "from urllib import request\n",
    "from concurrent import futures\n",
    "# from google.colab import files\n",
    "import json\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import py3Dmol\n",
    "\n",
    "from alphafold.model import model\n",
    "from alphafold.model import config\n",
    "from alphafold.model import data\n",
    "\n",
    "from alphafold.data import parsers\n",
    "from alphafold.data import pipeline\n",
    "from alphafold.data.tools import jackhmmer\n",
    "\n",
    "from alphafold.common import protein\n",
    "\n",
    "if use_amber_relax:\n",
    "  from alphafold.relax import relax\n",
    "  from alphafold.relax import utils\n",
    "\n",
    "def run_jackhmmer(sequence, prefix):\n",
    "\n",
    "  fasta_path = f\"{prefix}.fasta\"\n",
    "  with open(fasta_path, 'wt') as f:\n",
    "    f.write(f'>query\\n{sequence}')\n",
    "\n",
    "  pickled_msa_path = f\"{prefix}.jackhmmer.pickle\"\n",
    "  if os.path.isfile(pickled_msa_path):\n",
    "    msas_dict = pickle.load(open(pickled_msa_path,\"rb\"))\n",
    "    msas, deletion_matrices, names = (msas_dict[k] for k in ['msas', 'deletion_matrices', 'names'])\n",
    "    full_msa = []\n",
    "    for msa in msas:\n",
    "      full_msa += msa\n",
    "  else:\n",
    "    # --- Find the closest source ---\n",
    "    test_url_pattern = 'https://storage.googleapis.com/alphafold-colab{:s}/latest/uniref90_2021_03.fasta.1'\n",
    "    ex = futures.ThreadPoolExecutor(3)\n",
    "    def fetch(source):\n",
    "      request.urlretrieve(test_url_pattern.format(source))\n",
    "      return source\n",
    "    fs = [ex.submit(fetch, source) for source in ['', '-europe', '-asia']]\n",
    "    source = None\n",
    "    for f in futures.as_completed(fs):\n",
    "      source = f.result()\n",
    "      ex.shutdown()\n",
    "      break\n",
    "\n",
    "    jackhmmer_binary_path = '/usr/bin/jackhmmer'\n",
    "    dbs = []\n",
    "\n",
    "    num_jackhmmer_chunks = {'uniref90': 59, 'smallbfd': 17, 'mgnify': 71}\n",
    "    total_jackhmmer_chunks = sum(num_jackhmmer_chunks.values())\n",
    "    with tqdm.notebook.tqdm(total=total_jackhmmer_chunks, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
    "      def jackhmmer_chunk_callback(i):\n",
    "        pbar.update(n=1)\n",
    "\n",
    "      pbar.set_description('Searching uniref90')\n",
    "      jackhmmer_uniref90_runner = jackhmmer.Jackhmmer(\n",
    "          binary_path=jackhmmer_binary_path,\n",
    "          database_path=f'https://storage.googleapis.com/alphafold-colab{source}/latest/uniref90_2021_03.fasta',\n",
    "          get_tblout=True,\n",
    "          num_streamed_chunks=num_jackhmmer_chunks['uniref90'],\n",
    "          streaming_callback=jackhmmer_chunk_callback,\n",
    "          z_value=135301051)\n",
    "      dbs.append(('uniref90', jackhmmer_uniref90_runner.query(fasta_path)))\n",
    "\n",
    "      pbar.set_description('Searching smallbfd')\n",
    "      jackhmmer_smallbfd_runner = jackhmmer.Jackhmmer(\n",
    "          binary_path=jackhmmer_binary_path,\n",
    "          database_path=f'https://storage.googleapis.com/alphafold-colab{source}/latest/bfd-first_non_consensus_sequences.fasta',\n",
    "          get_tblout=True,\n",
    "          num_streamed_chunks=num_jackhmmer_chunks['smallbfd'],\n",
    "          streaming_callback=jackhmmer_chunk_callback,\n",
    "          z_value=65984053)\n",
    "      dbs.append(('smallbfd', jackhmmer_smallbfd_runner.query(fasta_path)))\n",
    "\n",
    "      pbar.set_description('Searching mgnify')\n",
    "      jackhmmer_mgnify_runner = jackhmmer.Jackhmmer(\n",
    "          binary_path=jackhmmer_binary_path,\n",
    "          database_path=f'https://storage.googleapis.com/alphafold-colab{source}/latest/mgy_clusters_2019_05.fasta',\n",
    "          get_tblout=True,\n",
    "          num_streamed_chunks=num_jackhmmer_chunks['mgnify'],\n",
    "          streaming_callback=jackhmmer_chunk_callback,\n",
    "          z_value=304820129)\n",
    "      dbs.append(('mgnify', jackhmmer_mgnify_runner.query(fasta_path)))\n",
    "\n",
    "    # --- Extract the MSAs and visualize ---\n",
    "    # Extract the MSAs from the Stockholm files.\n",
    "    # NB: deduplication happens later in pipeline.make_msa_features.\n",
    "\n",
    "    mgnify_max_hits = 501\n",
    "    msas = []\n",
    "    deletion_matrices = []\n",
    "    names = []\n",
    "    for db_name, db_results in dbs:\n",
    "      unsorted_results = []\n",
    "      for i, result in enumerate(db_results):\n",
    "        msa, deletion_matrix, target_names = parsers.parse_stockholm(result['sto'])\n",
    "        e_values_dict = parsers.parse_e_values_from_tblout(result['tbl'])\n",
    "        e_values = [e_values_dict[t.split('/')[0]] for t in target_names]\n",
    "        zipped_results = zip(msa, deletion_matrix, target_names, e_values)\n",
    "        if i != 0:\n",
    "          # Only take query from the first chunk\n",
    "          zipped_results = [x for x in zipped_results if x[2] != 'query']\n",
    "        unsorted_results.extend(zipped_results)\n",
    "      sorted_by_evalue = sorted(unsorted_results, key=lambda x: x[3])\n",
    "      db_msas, db_deletion_matrices, db_names, _ = zip(*sorted_by_evalue)\n",
    "      if db_msas:\n",
    "        if db_name == 'mgnify':\n",
    "          db_msas = db_msas[:mgnify_max_hits]\n",
    "          db_deletion_matrices = db_deletion_matrices[:mgnify_max_hits]\n",
    "          db_names = db_names[:mgnify_max_hits]\n",
    "        msas.append(db_msas)\n",
    "        deletion_matrices.append(db_deletion_matrices)\n",
    "        names.append(db_names)\n",
    "        msa_size = len(set(db_msas))\n",
    "        print(f'{msa_size} Sequences Found in {db_name}')\n",
    "\n",
    "      pickle.dump({\"msas\":msas,\n",
    "                   \"deletion_matrices\":deletion_matrices,\n",
    "                   \"names\":names}, open(pickled_msa_path,\"wb\"))\n",
    "  return msas, deletion_matrices, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rowN0bVYLe9n"
   },
   "outputs": [],
   "source": [
    "#@title Enter the amino acid sequence to fold ⬇️\n",
    "import re\n",
    "\n",
    "# define sequence\n",
    "sequence = 'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK' #@param {type:\"string\"}\n",
    "sequence = re.sub(\"[^A-Z:/]\", \"\", sequence.upper())\n",
    "sequence = re.sub(\":+\",\":\",sequence)\n",
    "sequence = re.sub(\"/+\",\"/\",sequence)\n",
    "\n",
    "# define number of copies\n",
    "homooligomer =  \"1\" #@param {type:\"string\"}\n",
    "if len(homooligomer) == 0: homooligomer = \"1\"\n",
    "homooligomer = re.sub(\"[^0-9:]\", \"\", homooligomer)\n",
    "homooligomers = [int(h) for h in homooligomer.split(\":\")]\n",
    "\n",
    "#@markdown - `sequence` Specify protein sequence to be modelled.\n",
    "#@markdown - `homooligomer` Define number of copies in a homo-oligomeric assembly.\n",
    "\n",
    "#@markdown ### Advanced options\n",
    "#@markdown - Within `sequence`, use `/` to specify chainbreaks within protein (can be used for trimming disordered regions).\n",
    "#@markdown - Within `sequence`, use `:` to specify chainbreaks between proteins (used for modeling complexes).\n",
    "#@markdown - For example, sequence `AC/DE:FGH` will be modelled as disjointed polypeptides: `AC`, `DE` and `FGH`. A seperate MSA will be generates for `ACDE` and `FGH`. If `pair_msa` is enabled, `ACDE`'s MSA will be paired with `FGH`'s MSA.\n",
    "\n",
    "#@markdown - Within `homooligomer`, use `:` to specify different homooligomeric state (copy numer) for each component of the complex. \n",
    "#@markdown - For example, **sequence:**`ABC:DEF`, **homooligomer:** `2:1`, the first protein `ABC` will be modeled as a homodimer (2 copies) and second `DEF` a monomer (1 copy).\n",
    "\n",
    "ori_sequence = sequence\n",
    "sequence = sequence.replace(\"/\",\"\").replace(\":\",\"\")\n",
    "seqs = ori_sequence.replace(\"/\",\"\").split(\":\")\n",
    "\n",
    "if len(seqs) != len(homooligomers):\n",
    "  if len(homooligomers) == 1:\n",
    "    homooligomers = [homooligomers[0]] * len(seqs)\n",
    "    homooligomer = \":\".join([str(h) for h in homooligomers])\n",
    "  else:\n",
    "    while len(seqs) > len(homooligomers):\n",
    "      homooligomers.append(1)\n",
    "    homooligomers = homooligomers[:len(seqs)]\n",
    "    homooligomer = \":\".join([str(h) for h in homooligomers])\n",
    "    print(\"WARNING: Mismatch between number of breaks ':' in 'sequence' and 'homooligomer' definition\")\n",
    "\n",
    "full_sequence = \"\".join([s*h for s,h in zip(seqs,homooligomers)])\n",
    "\n",
    "# prediction directory\n",
    "output_dir = 'prediction_' + cf.get_hash(full_sequence)[:5]\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# delete existing files in working directory\n",
    "for f in os.listdir(output_dir):\n",
    "  os.remove(os.path.join(output_dir, f))\n",
    "\n",
    "MIN_SEQUENCE_LENGTH = 16\n",
    "MAX_SEQUENCE_LENGTH = 2500\n",
    "\n",
    "aatypes = set('ACDEFGHIKLMNPQRSTVWY')  # 20 standard aatypes\n",
    "if not set(full_sequence).issubset(aatypes):\n",
    "  raise Exception(f'Input sequence contains non-amino acid letters: {set(sequence) - aatypes}. AlphaFold only supports 20 standard amino acids as inputs.')\n",
    "if len(full_sequence) < MIN_SEQUENCE_LENGTH:\n",
    "  raise Exception(f'Input sequence is too short: {len(full_sequence)} amino acids, while the minimum is {MIN_SEQUENCE_LENGTH}')\n",
    "if len(full_sequence) > MAX_SEQUENCE_LENGTH:\n",
    "  raise Exception(f'Input sequence is too long: {len(full_sequence)} amino acids, while the maximum is {MAX_SEQUENCE_LENGTH}. Please use the full AlphaFold system for long sequences.')\n",
    "\n",
    "if len(full_sequence) > 1400:\n",
    "  print(f\"WARNING: For a typical Google-Colab-GPU (16G) session, the max total length is ~1400 residues. You are at {len(full_sequence)}! Run Alphafold may crash.\")\n",
    "\n",
    "print(f\"homooligomer: '{homooligomer}'\")\n",
    "print(f\"total_length: '{len(full_sequence)}'\")\n",
    "print(f\"working_directory: '{output_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ITcPnLkLuDDE"
   },
   "outputs": [],
   "source": [
    "#@title Search against genetic databases\n",
    "#@markdown Once this cell has been executed, you will see\n",
    "#@markdown statistics about the multiple sequence alignment \n",
    "#@markdown (MSA) that will be used by AlphaFold. In particular, \n",
    "#@markdown you’ll see how well each residue is covered by similar \n",
    "#@markdown sequences in the MSA.\n",
    "#@markdown (Note that the search against databases and the actual prediction can take some time, from minutes to hours, depending on the length of the protein and what type of GPU you are allocated by Colab.)\n",
    "\n",
    "#@markdown ---\n",
    "msa_method = \"mmseqs2\" #@param [\"mmseqs2\",\"jackhmmer\",\"single_sequence\",\"custom_a3m\",\"precomputed\"]\n",
    "#@markdown - `mmseqs2` - FAST method from [ColabFold](https://github.com/sokrypton/ColabFold)\n",
    "#@markdown - `jackhmmer` - default method from Deepmind (SLOW, but may find more/less sequences).\n",
    "#@markdown - `single_sequence` - use single sequence input (not recommended, unless a *denovo* design and you dont expect to find any homologous sequences)\n",
    "#@markdown - `custom_a3m` Upload custom MSA (a3m format)\n",
    "#@markdown - `precomputed` If you have previously run this notebook and saved the results,\n",
    "#@markdown you can skip this step by uploading \n",
    "#@markdown the previously generated  `prediction_?????/msa.pickle`\n",
    "pair_msa = False #@param {type:\"boolean\"}\n",
    "cov = 0 #@param [\"0\",\"25\",\"50\",\"75\",\"90\"] {type:\"raw\"}\n",
    "#@markdown - `pair_msa` experimental option, that attempts to pair sequences from the same genome/operon. (Note: Will only help if a prokaryotic protein complex). \n",
    "#@markdown - `cov` filter to remove sequences that don't cover at least `cov` % of query. (Set to `0` to disable all fitlering.)\n",
    "\n",
    "# --- Search against genetic databases ---\n",
    "if msa_method == \"precomputed\":\n",
    "  print(\"upload precomputed pickled msa from previous run\")\n",
    "  pickled_msa_dict = files.upload()\n",
    "  msas_dict = pickle.loads(pickled_msa_dict[list(pickled_msa_dict.keys())[0]])\n",
    "  msas, deletion_matrices = (msas_dict[k] for k in ['msas', 'deletion_matrices'])\n",
    "\n",
    "elif msa_method == \"single_sequence\":\n",
    "  msas = [[sequence]]\n",
    "  deletion_matrices = [[[0]*len(sequence)]]\n",
    "\n",
    "elif msa_method == \"custom_a3m\":\n",
    "  print(\"upload custom a3m\")\n",
    "  msa_dict = files.upload()\n",
    "  lines = msa_dict[list(msa_dict.keys())[0]].decode().splitlines()\n",
    "  a3m_lines = []\n",
    "  for line in lines:\n",
    "    line = line.replace(\"\\x00\",\"\")\n",
    "    if len(line) > 0 and not line.startswith('#'):\n",
    "      a3m_lines.append(line)\n",
    "  msa, deletion_matrix = parsers.parse_a3m(\"\\n\".join(a3m_lines))\n",
    "  msas,deletion_matrices = [msa],[deletion_matrix]\n",
    "\n",
    "  if len(msas[0][0]) != len(sequence):\n",
    "    print(\"ERROR: the length of msa does not match input sequence\")\n",
    "\n",
    "else:\n",
    "  os.makedirs('tmp', exist_ok=True)\n",
    "  seqs = ori_sequence.replace('/','').split(':')\n",
    "\n",
    "  _blank_seq = [\"-\" * len(seq) for seq in seqs]\n",
    "  _blank_mtx = [[0] * len(seq) for seq in seqs]\n",
    "  def _pad(ns,vals,mode):\n",
    "    if mode == \"seq\": _blank = _blank_seq.copy()\n",
    "    if mode == \"mtx\": _blank = _blank_mtx.copy()\n",
    "    if isinstance(ns, list):\n",
    "      for n,val in zip(ns,vals): _blank[n] = val\n",
    "    else: _blank[ns] = vals\n",
    "    if mode == \"seq\": return \"\".join(_blank)\n",
    "    if mode == \"mtx\": return sum(_blank,[])\n",
    "\n",
    "  # gather msas\n",
    "  msas, deletion_matrices = [],[]\n",
    "  if msa_method == \"mmseqs2\":\n",
    "    prefix = cf.get_hash(\"\".join(seqs))\n",
    "    prefix = os.path.join('tmp',prefix)\n",
    "    print(f\"running mmseqs2\")\n",
    "    A3M_LINES = cf.run_mmseqs2(seqs, prefix, filter=True)\n",
    "\n",
    "  for n, seq in enumerate(seqs):\n",
    "    # tmp directory\n",
    "    prefix = cf.get_hash(seq)\n",
    "    prefix = os.path.join('tmp',prefix)\n",
    "\n",
    "    if msa_method == \"mmseqs2\":\n",
    "      # run mmseqs2\n",
    "      a3m_lines = A3M_LINES[n]\n",
    "      msa, mtx = parsers.parse_a3m(a3m_lines)\n",
    "      msas_, mtxs_ = [msa],[mtx]\n",
    "\n",
    "    elif msa_method == \"jackhmmer\":\n",
    "      print(f\"running jackhmmer on seq_{n}\")\n",
    "      # run jackhmmer\n",
    "      msas_, mtxs_, names_ = run_jackhmmer(seq, prefix)\n",
    "    \n",
    "    # pad sequences\n",
    "    for msa_,mtx_ in zip(msas_,mtxs_):\n",
    "      msa,mtx = [sequence],[[0]*len(sequence)]      \n",
    "      for s,m in zip(msa_,mtx_):\n",
    "        msa.append(_pad(n,s,\"seq\"))\n",
    "        mtx.append(_pad(n,m,\"mtx\"))\n",
    "\n",
    "      msas.append(msa)\n",
    "      deletion_matrices.append(mtx)\n",
    "\n",
    "  ####################################################################################\n",
    "  ####################################################################################\n",
    "\n",
    "  if pair_msa and len(seqs) > 1:\n",
    "    print(\"attempting to pair some sequences...\")\n",
    "\n",
    "    if msa_method == \"mmseqs2\":\n",
    "      prefix = cf.get_hash(\"\".join(seq))\n",
    "      prefix = os.path.join('tmp',prefix)\n",
    "      print(f\"running mmseqs2_noenv_nofilter on all seqs\")\n",
    "      A3M_LINES = cf.run_mmseqs2(seqs, prefix, use_env=False, filter=False)\n",
    "\n",
    "    _data = []\n",
    "    for a in range(len(seqs)):\n",
    "      _seq = seqs[a]\n",
    "      _prefix = os.path.join('tmp',cf.get_hash(_seq))\n",
    "\n",
    "      if msa_method == \"mmseqs2\":\n",
    "        a3m_lines = A3M_LINES[a]\n",
    "        _msa, _mtx, _lab = pairmsa.parse_a3m(a3m_lines)\n",
    "\n",
    "      elif msa_method == \"jackhmmer\":\n",
    "        _msas, _mtxs, _names = run_jackhmmer(_seq, _prefix)\n",
    "        _msa, _mtx, _lab = pairmsa.get_uni_jackhmmer(_msas[0], _mtxs[0], _names[0])\n",
    "\n",
    "      if len(_msa) > 1:\n",
    "        _data.append(pairmsa.hash_it(_msa, _lab, _mtx, call_uniprot=False))\n",
    "      else:\n",
    "        _data.append(None)\n",
    "    \n",
    "    for a in range(len(seqs)):\n",
    "      if _data[a] is not None:\n",
    "        for b in range(a+1,len(seqs)):\n",
    "          if _data[b] is not None:\n",
    "            _seq_a, _seq_b, _mtx_a, _mtx_b = pairmsa.stitch(_data[a],_data[b])\n",
    "            print(f\"attempting to pair seq_{a} and seq_{b}... FOUND: {len(_seq_a)}\")            \n",
    "            if len(_seq_a) > 0:\n",
    "              msa,mtx = [sequence],[[0]*len(sequence)]\n",
    "              for s_a,s_b,m_a,m_b in zip(_seq_a, _seq_b, _mtx_a, _mtx_b):\n",
    "                msa.append(_pad([a,b],[s_a,s_b],\"seq\"))\n",
    "                mtx.append(_pad([a,b],[m_a,m_b],\"mtx\"))\n",
    "              msas.append(msa)\n",
    "              deletion_matrices.append(mtx)\n",
    "\n",
    "####################################################################################\n",
    "####################################################################################\n",
    "  \n",
    "# save MSA as pickle\n",
    "pickle.dump({\"msas\":msas,\"deletion_matrices\":deletion_matrices},\n",
    "            open(os.path.join(output_dir,\"msa.pickle\"),\"wb\"))\n",
    "\n",
    "if msa_method != \"single_sequence\" and cov > 0:\n",
    "  # filter sequences that don't cover at least % \n",
    "  msas, deletion_matrices = cf.cov_filter(msas, deletion_matrices, cov)\n",
    "      \n",
    "full_msa = []\n",
    "for msa in msas: full_msa += msa\n",
    "\n",
    "# deduplicate\n",
    "deduped_full_msa = list(dict.fromkeys(full_msa))\n",
    "total_msa_size = len(deduped_full_msa)\n",
    "if msa_method == \"mmseqs2\":\n",
    "  print(f'\\n{total_msa_size} Sequences Found in Total (after filtering)\\n')\n",
    "else:\n",
    "  print(f'\\n{total_msa_size} Sequences Found in Total\\n')\n",
    "\n",
    "msa_arr = np.array([list(seq) for seq in deduped_full_msa])\n",
    "num_alignments, num_res = msa_arr.shape\n",
    "\n",
    "if num_alignments > 1:\n",
    "  plt.figure(figsize=(8,5),dpi=100)\n",
    "  plt.title(\"Sequence coverage\")\n",
    "  seqid = (np.array(list(sequence)) == msa_arr).mean(-1)\n",
    "  seqid_sort = seqid.argsort() #[::-1]\n",
    "  non_gaps = (msa_arr != \"-\").astype(float)\n",
    "  non_gaps[non_gaps == 0] = np.nan\n",
    "  plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
    "            interpolation='nearest', aspect='auto',\n",
    "            cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower',\n",
    "            extent=(0, msa_arr.shape[1], 0, msa_arr.shape[0]))\n",
    "  plt.plot((msa_arr != \"-\").sum(0), color='black')\n",
    "  plt.xlim(0,msa_arr.shape[1])\n",
    "  plt.ylim(0,msa_arr.shape[0])\n",
    "  plt.colorbar(label=\"Sequence identity to query\",)\n",
    "  plt.xlabel(\"Positions\")\n",
    "  plt.ylabel(\"Sequences\")\n",
    "  plt.savefig(os.path.join(output_dir,\"msa_coverage.png\"), bbox_inches = 'tight', dpi=200)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bQe3KeyTcv0n"
   },
   "outputs": [],
   "source": [
    "#@title run alphafold\n",
    "#@markdown ---\n",
    "rank_by = \"pLDDT\" #@param [\"pLDDT\",\"pTMscore\"]\n",
    "relax_all = False #@param {type:\"boolean\"}\n",
    "use_ptm = True #@param {type:\"boolean\"}\n",
    "use_turbo = True #@param {type:\"boolean\"}\n",
    "max_msa = \"512:1024\" #@param [\"512:1024\", \"256:512\", \"128:256\", \"64:128\", \"32:64\"]\n",
    "max_msa_clusters, max_extra_msa = [int(x) for x in max_msa.split(\":\")]\n",
    "#@markdown - `rank_by` specify metric to use for ranking models (For protein-protein complexes, we recommend pTMscore)\n",
    "#@markdown - `relax_all` amber-relax all models. Disable to only relax the top ranked model. (Note: no models are relaxed if `use_amber_relax` is disabled)\n",
    "#@markdown - `use_ptm` uses Deepmind's `ptm` finetuned model parameters to get PAE per structure. Disable to use the original model params.\n",
    "#@markdown - `use_turbo` introduces a few modifications (compile once, swap params, adjust max_msa) to speedup and reduce memory requirements. Disable for default behavior.\n",
    "#@markdown - `max_msa` defines: `max_msa_clusters:max_extra_msa` number of sequences to use. When adjusting after GPU crash, be sure to `Runtime` → `Restart runtime`. (Lowering will reduce GPU requirements, but may result in poor model quality. This option ignored if `use_turbo` is disabled)\n",
    "\n",
    "#@markdown Sampling options\n",
    "#@markdown ---\n",
    "#@markdown There are two stochastic parts of the pipeline. Within the feature generation (choice of cluster centers) and within the model (dropout). \n",
    "#@markdown To get structure diversity, you can iterate through a fixed number of random_seeds (using `num_samples`) and/or enable dropout (using `is_training`).\n",
    "\n",
    "num_models = 5 #@param [1,2,3,4,5] {type:\"raw\"}\n",
    "num_ensemble = 1 #@param [1,8] {type:\"raw\"}\n",
    "max_recycles = 3 #@param [1,3,6,12,24,48] {type:\"raw\"}\n",
    "tol = 0 #@param [0,0.1,0.5,1] {type:\"raw\"}\n",
    "is_training = False #@param {type:\"boolean\"}\n",
    "num_samples = 1 #@param [1,2,4,8,16,32] {type:\"raw\"}\n",
    "#@markdown - `num_models` specify how many model params to try. (5 recommended)\n",
    "#@markdown - `num_ensemble` the trunk of the network is run multiple times with different random choices for the MSA cluster centers. (`1`=`default`, `8`=`casp14 setting`)\n",
    "#@markdown - `max_recycles` controls the maximum number of times the structure is fed back into the neural network for refinement. (3 recommended)\n",
    "#@markdown - `tol` tolerance for deciding when to stop (CA-RMS between recycles)\n",
    "#@markdown - `is_training` enables the stochastic part of the model (dropout), when coupled with `num_samples` can be used to \"sample\" a diverse set of structures.\n",
    "#@markdown - `num_samples` number of random_seeds to try.\n",
    "\n",
    "if use_ptm == False and rank_by == \"pTMscore\":\n",
    "  print(\"WARNING: models will be ranked by pLDDT, 'use_ptm' is needed to compute pTMscore\")\n",
    "  rank_by = \"pLDDT\"\n",
    "\n",
    "#############################\n",
    "# delete old files\n",
    "#############################\n",
    "for f in os.listdir(output_dir):\n",
    "  if \"rank_\" in f:\n",
    "    os.remove(os.path.join(output_dir, f))\n",
    "\n",
    "#############################\n",
    "# homooligomerize\n",
    "#############################\n",
    "lengths = [len(seq) for seq in seqs]\n",
    "msas_mod, deletion_matrices_mod = cf.homooligomerize_heterooligomer(msas, deletion_matrices,\n",
    "                                                                    lengths, homooligomers)\n",
    "#############################\n",
    "# define input features\n",
    "#############################\n",
    "def _placeholder_template_feats(num_templates_, num_res_):\n",
    "  return {\n",
    "      'template_aatype': np.zeros([num_templates_, num_res_, 22], np.float32),\n",
    "      'template_all_atom_masks': np.zeros([num_templates_, num_res_, 37, 3], np.float32),\n",
    "      'template_all_atom_positions': np.zeros([num_templates_, num_res_, 37], np.float32),\n",
    "      'template_domain_names': np.zeros([num_templates_], np.float32),\n",
    "      'template_sum_probs': np.zeros([num_templates_], np.float32),\n",
    "  }\n",
    "\n",
    "num_res = len(full_sequence)\n",
    "feature_dict = {}\n",
    "feature_dict.update(pipeline.make_sequence_features(full_sequence, 'test', num_res))\n",
    "feature_dict.update(pipeline.make_msa_features(msas_mod, deletion_matrices=deletion_matrices_mod))\n",
    "if not use_turbo:\n",
    "  feature_dict.update(_placeholder_template_feats(0, num_res))\n",
    "\n",
    "################################\n",
    "# set chain breaks\n",
    "################################\n",
    "Ls = []\n",
    "for seq,h in zip(ori_sequence.split(\":\"),homooligomers):\n",
    "  Ls += [len(s) for s in seq.split(\"/\")] * h\n",
    "\n",
    "Ls_plot = sum([[len(seq)]*h for seq,h in zip(seqs,homooligomers)],[])\n",
    "\n",
    "feature_dict['residue_index'] = cf.chain_break(feature_dict['residue_index'], Ls)\n",
    "\n",
    "###########################\n",
    "# run alphafold\n",
    "###########################\n",
    "def parse_results(prediction_result, processed_feature_dict):\n",
    "  b_factors = prediction_result['plddt'][:,None] * prediction_result['structure_module']['final_atom_mask']\n",
    "  out = {\"unrelaxed_protein\": protein.from_prediction(processed_feature_dict, prediction_result, b_factors=b_factors),\n",
    "         \"plddt\": prediction_result['plddt'],\n",
    "         \"pLDDT\": prediction_result['plddt'].mean(),\n",
    "         \"dists\": prediction_result[\"distogram\"][\"bin_edges\"][prediction_result[\"distogram\"][\"logits\"].argmax(-1)],\n",
    "         \"adj\": jax.nn.softmax(prediction_result[\"distogram\"][\"logits\"])[:,:,prediction_result[\"distogram\"][\"bin_edges\"] < 8].sum(-1)}\n",
    "  if \"ptm\" in prediction_result:\n",
    "    out.update({\"pae\": prediction_result['predicted_aligned_error'],\n",
    "                \"pTMscore\": prediction_result['ptm']})\n",
    "  return out\n",
    "\n",
    "model_names = ['model_1', 'model_2', 'model_3', 'model_4', 'model_5'][:num_models]\n",
    "total = len(model_names) * num_samples\n",
    "if use_amber_relax:\n",
    "  if relax_all: total += total\n",
    "  else: total += 1\n",
    "\n",
    "save_pae_json = True #@param {type:\"boolean\"}\n",
    "show_images = True #@param {type:\"boolean\"}\n",
    "\n",
    "with tqdm.notebook.tqdm(total=total, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
    "  #######################################################################\n",
    "  # precompile model and recompile only if length changes\n",
    "  #######################################################################\n",
    "  if use_turbo:\n",
    "    name = \"model_5_ptm\" if use_ptm else \"model_5\"\n",
    "    N = len(feature_dict[\"msa\"])\n",
    "    L = len(feature_dict[\"residue_index\"])\n",
    "    compiled = (N, L, use_ptm, max_recycles, tol, num_ensemble, max_msa, is_training)\n",
    "    if \"COMPILED\" in dir():\n",
    "      if COMPILED != compiled: recompile = True\n",
    "    else: recompile = True\n",
    "    if recompile:\n",
    "      cf.clear_mem(\"gpu\")\n",
    "      cfg = config.model_config(name)      \n",
    "      cfg.data.common.max_extra_msa = min(N,max_extra_msa)\n",
    "      cfg.data.eval.max_msa_clusters = min(N,max_msa_clusters)\n",
    "      cfg.data.common.num_recycle = max_recycles\n",
    "      cfg.model.num_recycle = max_recycles\n",
    "      cfg.model.recycle_tol = tol\n",
    "      cfg.data.eval.num_ensemble = num_ensemble\n",
    "\n",
    "      params = data.get_model_haiku_params(name,'/opt/colabfold/alphafold-repo/data/')\n",
    "      model_runner = model.RunModel(cfg, params, is_training=is_training)\n",
    "      COMPILED = compiled\n",
    "      recompile = False\n",
    "\n",
    "  else:\n",
    "    cf.clear_mem(\"gpu\")\n",
    "    recompile = True\n",
    "\n",
    "  # cleanup\n",
    "  if \"outs\" in dir(): del outs\n",
    "  outs = {}\n",
    "  cf.clear_mem(\"cpu\")  \n",
    "\n",
    "  #######################################################################\n",
    "  for num, model_name in enumerate(model_names): # for each model\n",
    "    name = model_name+\"_ptm\" if use_ptm else model_name\n",
    "\n",
    "    # setup model and/or params\n",
    "    params = data.get_model_haiku_params(name, '/opt/colabfold/alphafold-repo/data')\n",
    "    if use_turbo:\n",
    "      for k in model_runner.params.keys():\n",
    "        model_runner.params[k] = params[k]\n",
    "    else:\n",
    "      cfg = config.model_config(name)\n",
    "      cfg.data.common.num_recycle = cfg.model.num_recycle = max_recycles\n",
    "      cfg.model.recycle_tol = tol\n",
    "      cfg.data.eval.num_ensemble = num_ensemble\n",
    "      model_runner = model.RunModel(cfg, params, is_training=is_training)\n",
    "\n",
    "    for seed in range(num_samples): # for each seed\n",
    "      # predict\n",
    "      key = f\"{name}_seed_{seed}\"\n",
    "      pbar.set_description(f'Running {key}')      \n",
    "      processed_feature_dict = model_runner.process_features(feature_dict, random_seed=seed)      \n",
    "      prediction_result, (r, t) = cf.to(model_runner.predict(processed_feature_dict, random_seed=seed),\"cpu\")\n",
    "      outs[key] = parse_results(prediction_result, processed_feature_dict)\n",
    "      \n",
    "      # report\n",
    "      pbar.update(n=1)\n",
    "      line = f\"{key} recycles:{r} tol:{t:.2f} pLDDT:{outs[key]['pLDDT']:.2f}\"\n",
    "      if use_ptm: line += f\" pTMscore:{outs[key]['pTMscore']:.2f}\"\n",
    "      print(line)\n",
    "      if show_images:\n",
    "        fig = cf.plot_protein(outs[key][\"unrelaxed_protein\"], Ls=Ls_plot, dpi=100)\n",
    "        plt.show()\n",
    "\n",
    "      # cleanup\n",
    "      del processed_feature_dict, prediction_result\n",
    "\n",
    "    if use_turbo:\n",
    "      del params\n",
    "    else:\n",
    "      del params, model_runner, cfg\n",
    "      cf.clear_mem(\"gpu\")\n",
    "\n",
    "  # delete old files\n",
    "  for f in os.listdir(output_dir):\n",
    "    if \"rank_\" in f:\n",
    "      os.remove(os.path.join(output_dir, f))\n",
    "\n",
    "  # Find the best model according to the mean pLDDT.\n",
    "  model_rank = list(outs.keys())\n",
    "  model_rank = [model_rank[i] for i in np.argsort([outs[x][rank_by] for x in model_rank])[::-1]]\n",
    "\n",
    "  # Write out the prediction\n",
    "  for n,key in enumerate(model_rank):\n",
    "    prefix = f\"rank_{n+1}_{key}\" \n",
    "    pred_output_path = os.path.join(output_dir,f'{prefix}_unrelaxed.pdb')\n",
    "    fig = cf.plot_protein(outs[key][\"unrelaxed_protein\"], Ls=Ls_plot, dpi=200)\n",
    "    plt.savefig(os.path.join(output_dir,f'{prefix}.png'), bbox_inches = 'tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    pdb_lines = protein.to_pdb(outs[key][\"unrelaxed_protein\"])\n",
    "    with open(pred_output_path, 'w') as f:\n",
    "      f.write(pdb_lines)\n",
    "    if use_amber_relax:\n",
    "      pbar.set_description(f'AMBER relaxation')\n",
    "      ! mkdir -p alphafold/common\n",
    "      ! wget -q -P alphafold/common https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
    "      if relax_all or n == 0:\n",
    "        amber_relaxer = relax.AmberRelaxation(\n",
    "            max_iterations=0,\n",
    "            tolerance=2.39,\n",
    "            stiffness=10.0,\n",
    "            exclude_residues=[],\n",
    "            max_outer_iterations=20)\n",
    "        relaxed_pdb_lines, _, _ = amber_relaxer.process(prot=outs[key][\"unrelaxed_protein\"])        \n",
    "        pred_output_path = os.path.join(output_dir,f'{prefix}_relaxed.pdb')\n",
    "        with open(pred_output_path, 'w') as f:\n",
    "          f.write(relaxed_pdb_lines)\n",
    "        pbar.update(n=1)\n",
    "      \n",
    "############################################################\n",
    "print(f\"model rank based on {rank_by}\")\n",
    "for n,key in enumerate(model_rank):\n",
    "  print(f\"rank_{n+1}_{key} {rank_by}:{outs[key][rank_by]:.2f}\")\n",
    "  if use_ptm and save_pae_json:\n",
    "    pae = outs[key][\"pae\"]\n",
    "    max_pae = pae.max()\n",
    "    # Save pLDDT and predicted aligned error (if it exists)\n",
    "    pae_output_path = os.path.join(output_dir,f'rank_{n+1}_{key}_pae.json')\n",
    "    # Save predicted aligned error in the same format as the AF EMBL DB\n",
    "    rounded_errors = np.round(np.asarray(pae), decimals=1)\n",
    "    indices = np.indices((len(rounded_errors), len(rounded_errors))) + 1\n",
    "    indices_1 = indices[0].flatten().tolist()\n",
    "    indices_2 = indices[1].flatten().tolist()\n",
    "    pae_data = json.dumps([{\n",
    "        'residue1': indices_1,\n",
    "        'residue2': indices_2,\n",
    "        'distance': rounded_errors.flatten().tolist(),\n",
    "        'max_predicted_aligned_error': max_pae.item()\n",
    "    }],\n",
    "                          indent=None,\n",
    "                          separators=(',', ':'))\n",
    "    with open(pae_output_path, 'w') as f:\n",
    "      f.write(pae_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KAZj6CBZTkJM"
   },
   "outputs": [],
   "source": [
    "#@title Display 3D structure {run: \"auto\"}\n",
    "rank_num = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\n",
    "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
    "show_sidechains = False #@param {type:\"boolean\"}\n",
    "show_mainchains = False #@param {type:\"boolean\"}\n",
    "\n",
    "key = model_rank[rank_num-1]\n",
    "prefix = f\"rank_{rank_num}_{key}\" \n",
    "pred_output_path = os.path.join(output_dir,f'{prefix}_relaxed.pdb')  \n",
    "if not os.path.isfile(pred_output_path):\n",
    "  pred_output_path = os.path.join(output_dir,f'{prefix}_unrelaxed.pdb') \n",
    "\n",
    "cf.show_pdb(pred_output_path, show_sidechains, show_mainchains, color, Ls=Ls_plot).show()\n",
    "if color == \"lDDT\": cf.plot_plddt_legend().show()  \n",
    "if use_ptm:\n",
    "  cf.plot_confidence(outs[key][\"plddt\"], outs[key][\"pae\"], Ls=Ls_plot).show()\n",
    "else:\n",
    "  cf.plot_confidence(outs[key][\"plddt\"], Ls=Ls_plot).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XzK2Wve12GCk"
   },
   "outputs": [],
   "source": [
    "#@title Extra plots\n",
    "dpi =  100#@param {type:\"integer\"}\n",
    "if use_ptm:\n",
    "  print(\"predicted alignment error\")\n",
    "  cf.plot_paes([outs[k][\"pae\"] for k in model_rank], Ls=Ls_plot, dpi=dpi)\n",
    "  plt.savefig(os.path.join(output_dir,f'predicted_alignment_error.png'), bbox_inches = 'tight', dpi=np.maximum(200,dpi))\n",
    "  plt.show()\n",
    "\n",
    "print(\"predicted contacts\")\n",
    "cf.plot_adjs([outs[k][\"adj\"] for k in model_rank], Ls=Ls_plot, dpi=dpi)\n",
    "plt.savefig(os.path.join(output_dir,f'predicted_contacts.png'), bbox_inches = 'tight', dpi=np.maximum(200,dpi))\n",
    "plt.show()\n",
    "\n",
    "print(\"predicted distogram\")\n",
    "cf.plot_dists([outs[k][\"dists\"] for k in model_rank], Ls=Ls_plot, dpi=dpi)\n",
    "plt.savefig(os.path.join(output_dir,f'predicted_distogram.png'), bbox_inches = 'tight', dpi=np.maximum(200,dpi))\n",
    "plt.show()\n",
    "\n",
    "print(\"predicted LDDT\")\n",
    "cf.plot_plddts([outs[k][\"plddt\"] for k in model_rank], Ls=Ls_plot, dpi=dpi)\n",
    "plt.savefig(os.path.join(output_dir,f'predicted_LDDT.png'), bbox_inches = 'tight', dpi=np.maximum(200,dpi))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Riekgf0KQv_3"
   },
   "outputs": [],
   "source": [
    "#@title Download prediction\n",
    "\n",
    "#@markdown Once this cell has been executed, a zip-archive with \n",
    "#@markdown the obtained prediction will be automatically downloaded \n",
    "#@markdown to your computer.\n",
    "\n",
    "# add settings file\n",
    "settings_path = os.path.join(output_dir,\"settings.txt\")\n",
    "with open(settings_path, \"w\") as text_file:\n",
    "  text_file.write(f\"notebook=https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb\\n\")\n",
    "  text_file.write(f\"sequence={ori_sequence}\\n\")\n",
    "  text_file.write(f\"msa_method={msa_method}\\n\")\n",
    "  text_file.write(f\"homooligomer={homooligomer}\\n\")\n",
    "  text_file.write(f\"pair_msa={pair_msa}\\n\")\n",
    "  text_file.write(f\"max_msa={max_msa}\\n\")\n",
    "  text_file.write(f\"cov={cov}\\n\")\n",
    "  text_file.write(f\"use_amber_relax={use_amber_relax}\\n\")\n",
    "  text_file.write(f\"use_turbo={use_turbo}\\n\")\n",
    "  text_file.write(f\"use_ptm={use_ptm}\\n\")\n",
    "  text_file.write(f\"rank_by={rank_by}\\n\")\n",
    "  text_file.write(f\"num_models={num_models}\\n\")\n",
    "  text_file.write(f\"num_samples={num_samples}\\n\")\n",
    "  text_file.write(f\"num_ensemble={num_ensemble}\\n\")\n",
    "  text_file.write(f\"max_recycles={max_recycles}\\n\")\n",
    "  text_file.write(f\"tol={tol}\\n\")\n",
    "  text_file.write(f\"is_training={is_training}\\n\")\n",
    "  text_file.write(f\"use_templates=False\\n\")\n",
    "  text_file.write(f\"-------------------------------------------------\\n\")\n",
    "\n",
    "  for n,key in enumerate(model_rank):\n",
    "    line = f\"rank_{n+1}_{key} pLDDT:{outs[key]['pLDDT']:.2f}\" + f\" pTMscore:{outs[key]['pTMscore']:.4f}\" if use_ptm else \"\"\n",
    "    text_file.write(line+\"\\n\")\n",
    "\n",
    "# --- Download the predictions ---\n",
    "!tar czf {output_dir}.tar.gz {output_dir}\n",
    "# files.download(f'{output_dir}.zip')\n",
    "\n",
    "print(\"Download prediction results:\")\n",
    "display(FileLink(f\"{output_dir}.tar.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "AlphaFold2_advanced.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
